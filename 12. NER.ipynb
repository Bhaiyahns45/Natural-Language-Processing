{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down all the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael Bloomberg | PERSON | People, including fictional\n",
      "Bloomberg | PERSON | People, including fictional\n",
      "1982 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = nlp(\"Michael Bloomberg founded Bloomberg in 1982\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  0 | 9\n",
      "Twitter Inc  |  ORG  |  30 | 41\n",
      "$45 billion  |  MONEY  |  46 | 57\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting custom entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  PRODUCT\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going to acquire"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = doc[2:5]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
    "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
    "\n",
    "doc.set_ents([s1, s2], default=\"unmodified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  ORG\n",
      "Twitter  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhaiya Singh  |  PERSON\n",
      "5689745841  |  DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bhaiya Singh works in nagarro, mobile nos is 5689745841\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new entity ruler\n",
    "ruler = EntityRuler(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the regular expression pattern for a mobile phone number\n",
    "phone_pattern = re.compile(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Pattern is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m patterns \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPHONE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m\"\u001b[39m: [{\u001b[39m\"\u001b[39m\u001b[39mTEXT\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mREGEX\u001b[39m\u001b[39m\"\u001b[39m: phone_pattern}}]}]\n\u001b[0;32m      4\u001b[0m \u001b[39m# add the patterns to the ruler\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m ruler\u001b[39m.\u001b[39;49madd_patterns(patterns)\n\u001b[0;32m      7\u001b[0m \u001b[39m# add the ruler to the pipeline\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(ruler)\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\pipeline\\entityruler.py:342\u001b[0m, in \u001b[0;36mEntityRuler.add_patterns\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pattern, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_patterns[label]\u001b[39m.\u001b[39mappend(pattern)\n\u001b[1;32m--> 342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatcher\u001b[39m.\u001b[39;49madd(label, [pattern])\n\u001b[0;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE097\u001b[39m.\u001b[39mformat(pattern\u001b[39m=\u001b[39mpattern))\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:135\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:789\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._preprocess_pattern\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:1049\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._get_extra_predicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:1080\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._get_extra_predicates_dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:877\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._RegexPredicate.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:833\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._predicate_cache_key\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\srsly\\_json_api.py:22\u001b[0m, in \u001b[0;36mjson_dumps\u001b[1;34m(data, indent, sort_keys)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m sort_keys:\n\u001b[0;32m     21\u001b[0m     indent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m indent \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m indent\n\u001b[1;32m---> 22\u001b[0m     result \u001b[39m=\u001b[39m _builtin_json\u001b[39m.\u001b[39;49mdumps(\n\u001b[0;32m     23\u001b[0m         data, indent\u001b[39m=\u001b[39;49mindent, separators\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m), sort_keys\u001b[39m=\u001b[39;49msort_keys\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     result \u001b[39m=\u001b[39m ujson\u001b[39m.\u001b[39mdumps(data, indent\u001b[39m=\u001b[39mindent, escape_forward_slashes\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Pattern is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the pattern to match using the regular expression for phone numbers\n",
    "patterns = [{\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": phone_pattern}}]}]\n",
    "\n",
    "# add the patterns to the ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# add the ruler to the pipeline\n",
    "nlp.add_pipe(ruler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}\n",
    "                                                        ]\n",
    "                }\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Pattern is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m phone_entity \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPHONE\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m\"\u001b[39m: [{\u001b[39m\"\u001b[39m\u001b[39mTEXT\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mREGEX\u001b[39m\u001b[39m\"\u001b[39m: phone_pattern}}]}\n\u001b[0;32m     27\u001b[0m \u001b[39m# add the pattern to the entity ruler\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m ruler\u001b[39m.\u001b[39;49madd_patterns([phone_entity])\n\u001b[0;32m     30\u001b[0m \u001b[39m# add the entity ruler to the pipeline\u001b[39;00m\n\u001b[0;32m     31\u001b[0m nlp\u001b[39m.\u001b[39madd_pipe(ruler)\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\pipeline\\entityruler.py:342\u001b[0m, in \u001b[0;36mEntityRuler.add_patterns\u001b[1;34m(self, patterns)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pattern, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_patterns[label]\u001b[39m.\u001b[39mappend(pattern)\n\u001b[1;32m--> 342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatcher\u001b[39m.\u001b[39;49madd(label, [pattern])\n\u001b[0;32m    343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE097\u001b[39m.\u001b[39mformat(pattern\u001b[39m=\u001b[39mpattern))\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:135\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:789\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._preprocess_pattern\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:1049\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._get_extra_predicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:1080\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._get_extra_predicates_dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:877\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._RegexPredicate.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\spacy\\matcher\\matcher.pyx:833\u001b[0m, in \u001b[0;36mspacy.matcher.matcher._predicate_cache_key\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\bhaiyasingh\\Documents\\NLP\\nlp_env\\lib\\site-packages\\srsly\\_json_api.py:22\u001b[0m, in \u001b[0;36mjson_dumps\u001b[1;34m(data, indent, sort_keys)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m sort_keys:\n\u001b[0;32m     21\u001b[0m     indent \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m indent \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m indent\n\u001b[1;32m---> 22\u001b[0m     result \u001b[39m=\u001b[39m _builtin_json\u001b[39m.\u001b[39;49mdumps(\n\u001b[0;32m     23\u001b[0m         data, indent\u001b[39m=\u001b[39;49mindent, separators\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m\"\u001b[39;49m), sort_keys\u001b[39m=\u001b[39;49msort_keys\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     result \u001b[39m=\u001b[39m ujson\u001b[39m.\u001b[39mdumps(data, indent\u001b[39m=\u001b[39mindent, escape_forward_slashes\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\users\\bhaiyasingh\\appdata\\local\\programs\\python\\python38\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Pattern is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import json\n",
    "\n",
    "# define the custom encoder\n",
    "class PatternEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, re.Pattern):\n",
    "            return obj.pattern\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# load the Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# define the regular expression pattern for phone numbers\n",
    "phone_pattern = re.compile(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b')\n",
    "\n",
    "# add the new entity to the model using the entity ruler\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# create a new entity ruler\n",
    "ruler = EntityRuler(nlp)\n",
    "\n",
    "# create the pattern for the new entity\n",
    "phone_entity = {\"label\": \"PHONE\", \"pattern\": [{\"TEXT\": {\"REGEX\": phone_pattern}}]}\n",
    "\n",
    "# add the pattern to the entity ruler\n",
    "ruler.add_patterns([phone_entity])\n",
    "\n",
    "# add the entity ruler to the pipeline\n",
    "nlp.add_pipe(ruler)\n",
    "\n",
    "# test the model with a sample text\n",
    "doc = nlp(\"Call me at 555-123-4567.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9271de32dda2a4aaf053193baeffae90b7d399b9418b7ee42583d54efe6a26a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
